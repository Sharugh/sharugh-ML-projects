{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa0nd+TmiPq36XZFdo7nDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharugh/sharugh-ML-projects/blob/main/Sharugh_Text_Analysis_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The goal of this code is to perform text analysis on articles obtained from a set of given URLs. The process involves two main steps: data extraction and data analysis**"
      ],
      "metadata": {
        "id": "UlOMUjl3PLDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download textblob**"
      ],
      "metadata": {
        "id": "yy-55Ta9PY_y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOoU1jlxZegw",
        "outputId": "e1791543-bd04-4861-f3d0-f7105a94c63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install Beautifulsoup4 , textblob , nltk**"
      ],
      "metadata": {
        "id": "sRID-hx-PsrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 textblob nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpTdm7lkZruk",
        "outputId": "872ac6d2-7b1a-462a-ed83-5c49092f080a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Extraction**"
      ],
      "metadata": {
        "id": "ljrzZtGoQmrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We start by reading the input data from the 'Input.xlsx' file using the 'pandas' library. This file contains a list of URLs along with corresponding URL IDs.\n",
        "\n",
        "Next, we use web scraping techniques with 'requests' and 'BeautifulSoup' to extract article titles and text from the HTML content of each URL. It's crucial to note that we focus on extracting only the article text, excluding headers, footers, and any irrelevant information. The extracted titles and text are then saved in separate text files, with filenames based on their respective URL IDs."
      ],
      "metadata": {
        "id": "UEKA9o0-QJSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "Read input file\n",
        "input_data = pd.read_excel(\"/content/Input.xlsx\")\n",
        "for index, row in input_data.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    article_title = soup.find('title').get_text()\n",
        "    article_text = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
        "    with open(f\"{url_id}.txt\", 'w', encoding='utf-8') as file:\n",
        "        file.write(f\"{article_title}\\n\\n{article_text}\")"
      ],
      "metadata": {
        "id": "j70h2IYPZyvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download cumdict**"
      ],
      "metadata": {
        "id": "607_uJFJRHtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRre6o2eaiok",
        "outputId": "6b094e98-d23c-414a-ab5a-c1c6e69ece04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Analysis:**\n",
        "\n",
        "### Moving on to the data analysis part, we employ the 'TextBlob' library for sentiment analysis and linguistic feature extraction. Additionally, we use the 'nltk' library for counting syllables.\n",
        "\n",
        "### To optimize the code's performance, we introduce parallel processing using the 'concurrent.futures' module. This helps speed up both the reading of text files and the text analysis itself.\n",
        "\n",
        "# **Output Data Structure:**\n",
        "\n",
        "### The results of our text analysis are organized into a DataFrame using 'pandas' and saved in the 'Output Data Structure.xlsx' file. This structured output includes various variables such as positive and negative scores, polarity, subjectivity, and other linguistic features."
      ],
      "metadata": {
        "id": "qfb27GVXR9Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from functools import partial\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import cmudict\n",
        "import pandas as pd\n",
        "def syllable_count(word):\n",
        "    d = cmudict.dict()\n",
        "    return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]) if word.lower() in d else 0\n",
        "def analyze_text(url_id, content):\n",
        "    blob = TextBlob(content)\n",
        "    words = blob.words\n",
        "    syllables = sum(syllable_count(word) for word in words)\n",
        "    positive_score = blob.sentiment.polarity\n",
        "    negative_score = 1 - positive_score\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "    avg_sentence_length = len(blob.sentences)\n",
        "    percentage_of_complex_words = len([word for word in words if syllable_count(word) > 2]) / len(words) * 100\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
        "    avg_number_of_words_per_sentence = len(words) / avg_sentence_length\n",
        "    complex_word_count = len([word for word in words if syllable_count(word) > 2])\n",
        "    word_count = len(words)\n",
        "    syllable_per_word = syllables / word_count\n",
        "    personal_pronouns = len([word for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']])\n",
        "    avg_word_length = sum(len(word) for word in words) / word_count\n",
        "\n",
        "    return [url_id, positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_of_complex_words, fog_index, avg_number_of_words_per_sentence,\n",
        "            complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word_length]\n",
        "def read_text_file(url_id):\n",
        "    with open(f\"{url_id}.txt\", 'r', encoding='utf-8') as file:\n",
        "        return url_id, file.read()\n",
        "input_data = pd.read_excel(\"/content/Input.xlsx\")\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    results = list(executor.map(partial(read_text_file), input_data['URL_ID']))\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    output_data = list(executor.map(partial(analyze_text), [result[0] for result in results], [result[1] for result in results]))\n",
        "columns = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "           'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "           'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "output_df = pd.DataFrame(output_data, columns=columns)\n",
        "output_df.to_excel(\"Output Data Structure.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "rSTY8By9FzZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion:**\n",
        "### In summary, this code efficiently extracts relevant information from a set of URLs, performs thorough text analysis, and organizes the results in a structured output format."
      ],
      "metadata": {
        "id": "o4rSPCv2S6iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Additional Points:**\n",
        "### We also implemented a syllable counting function using the CMU Pronouncing Dictionary to enhance our text analysis.\n",
        "\n",
        "### The introduction of parallel processing significantly improves the overall speed of the code, making it more efficient, especially when dealing with a large number of URLs."
      ],
      "metadata": {
        "id": "0H_TG-mVTLJ5"
      }
    }
  ]
}